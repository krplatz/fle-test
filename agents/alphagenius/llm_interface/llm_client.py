from abc import ABC, abstractmethod
import inspect # For generate_script_async default in BaseLLMClient
import asyncio # For ConcreteLLMClient sync wrapper and main test
import traceback # For ConcreteLLMClient error printing

# Attempt to import the real LLMFactory and parse_response
try:
    from agents.utils.llm_factory import LLMFactory
    from agents.utils.parse_response import parse_response # Real parser
except ImportError:
    print("Warning: Real LLMFactory or parse_response not found. Using placeholders for ConcreteLLMClient structure.")
    class LLMFactory: # Placeholder
        async def acall(self, messages, model, max_tokens, temperature):
            class MockChoice:
                class MockMessage:
                    content = "print('Mock LLM response from placeholder LLMFactory')"
                message = MockMessage()
            class MockResponse:
                choices = [MockChoice()]
            return MockResponse()
    
    def parse_response(response_obj): # Placeholder parser
        if hasattr(response_obj, 'choices') and response_obj.choices:
            return response_obj.choices[0].message.content
        return "print('Error: Could not parse LLM response with placeholder parse_response')"

class BaseLLMClient(ABC):
    @abstractmethod
    def generate_script(self, prompt: str) -> str:
        pass

    async def generate_script_async(self, prompt: str) -> str:
        if hasattr(self, 'generate_script') and not inspect.iscoroutinefunction(self.generate_script):
            return self.generate_script(prompt)
        raise NotImplementedError("Async script generation not implemented by this client.")


class PlaceholderLLMClient(BaseLLMClient):
    def generate_script(self, prompt: str) -> str:
        print(f"PlaceholderLLMClient: Received prompt: '{prompt}'")
        
        prompt_lower = prompt.lower()
        python_script = ""
        header = "# Python script generated by PlaceholderLLMClient\n"

        if "mine" in prompt_lower and "iron ore" in prompt_lower:
            quantity = 10 
            python_script = (
                f"print('Attempting to mine {quantity} iron ore.')\n"
                f"result = mine_resource('iron-ore', {quantity}) # Conceptual FLE tool call\n"
                f"print(f'Mining result: {{result}}')"
            )
        elif "craft" in prompt_lower and "furnace" in prompt_lower:
            python_script = (
                "print('Attempting to craft a stone furnace.')\n"
                "crafted_furnace = craft_item('stone-furnace') # Conceptual FLE tool call\n"
                "print(f'Crafting result: {{crafted_furnace}}')"
            )
        elif "research" in prompt_lower and "automation" in prompt_lower:
            python_script = (
                "print('Attempting to research automation.')\n"
                "research_status = set_research('automation') # Conceptual FLE tool call\n"
                "print(f'Research status: {{research_status}}')"
            )
        elif "place" in prompt_lower and "furnace" in prompt_lower:
            python_script = (
                "print('Attempting to place a stone furnace using Direction.NORTH.')\n"
                "# Conceptual: get player position first, then place next to player\n"
                "# For this test, ensure 'Direction' is available in sandbox_globals\n"
                "try:\n"
                "    furnace_pos = {'x': 10, 'y': 10} # Example position\n"
                "    # Using an Enum member for direction:\n"
                "    placed_entity = place_entity(entity_name='stone-furnace', position=furnace_pos, direction=Direction.NORTH) # Conceptual FLE tool call\n"
                "    print(f'Placement result: {{placed_entity}}')\n"
                "except NameError as e:\n"
                "    print(f'Error during place_entity: {{e}}. Is Direction Enum available?')\n"
                "except Exception as e:\n"
                "    print(f'An unexpected error occurred during place_entity: {{e}}')"
            )
        else:
            python_script = (
                "print('PlaceholderLLMClient: Received unhandled prompt - performing generic action.')\n"
                "generic_result = get_player_inventory() # Conceptual FLE tool call\n"
                "print(f'Generic action result (inventory): {{generic_result}}')"
            )
            
        python_script = header + python_script
        
        print(f"PlaceholderLLMClient: Returning Python script for prompt '{prompt}':\n---\n{python_script}\n---")
        return python_script

    async def generate_script_async(self, prompt: str) -> str:
        return self.generate_script(prompt)


class ConcreteLLMClient(BaseLLMClient):
    def __init__(self, llm_factory: LLMFactory, model_name: str = "gpt-4o-mini", default_system_prompt: str = "You are a helpful AI assistant."): # Added default_system_prompt
        self.llm_factory = llm_factory
        self.model_name = model_name
        self.default_system_prompt = default_system_prompt # Store default
        print(f"ConcreteLLMClient initialized with model: {self.model_name}, factory: {type(self.llm_factory).__name__}, default system prompt (first 50 chars): '{self.default_system_prompt[:50]}...'")

    async def generate_script_async(self, user_prompt_content: str, system_prompt_override: Optional[str] = None) -> str:
        current_system_prompt = system_prompt_override if system_prompt_override is not None else self.default_system_prompt
        
        print(f"ConcreteLLMClient: Using system prompt (first 100 chars): '{current_system_prompt[:100]}...'")
        print(f"ConcreteLLMClient: Using user prompt content (first 100 chars): '{user_prompt_content[:100]}...'")
        
        messages = [
            {"role": "system", "content": current_system_prompt},
            {"role": "user", "content": user_prompt_content}
        ]
        
        try:
            if not hasattr(self.llm_factory, 'acall') or not callable(self.llm_factory.acall):
                print("Error: LLMFactory instance does not have a callable 'acall' method.")
                return "print('Error: LLMFactory does not have acall method.')" # Return valid Python

            response_obj = await self.llm_factory.acall(
                messages=messages,
                model=self.model_name,
                max_tokens=1500, 
                temperature=0.3 
            )
            
            # In ConcreteLLMClient.generate_script_async, after response_obj = await self.llm_factory.acall(...)

            # Assuming parse_response is imported from agents.utils.parse_response
            policy_obj = parse_response(response_obj) 
            
            python_script = "" # Initialize

            if policy_obj and hasattr(policy_obj, 'code') and isinstance(policy_obj.code, str):
                python_script = policy_obj.code
                if not python_script.strip(): # Code attribute exists but is empty or whitespace
                     print(f"Warning: parse_response returned Policy object with empty/whitespace code string.")
                     python_script = "print('Error: LLM returned an empty code policy. No action taken.')" 
            else:
                # This case handles:
                # 1. policy_obj is None (parse_response failed to find code, e.g. LLM response had no ```python block)
                # 2. policy_obj does not have a 'code' attribute or it's not a string (unexpected Policy structure)
                
                llm_full_text_response_snippet = "Could not retrieve."
                try: # Try to get some raw text from the LLM for debugging
                    if policy_obj and hasattr(policy_obj, 'meta') and hasattr(policy_obj.meta, 'text_response') and policy_obj.meta.text_response:
                        llm_full_text_response_snippet = policy_obj.meta.text_response[:200] # Get from Policy's meta if available
                    elif hasattr(response_obj, 'choices') and response_obj.choices and hasattr(response_obj.choices[0], 'message') and hasattr(response_obj.choices[0].message, 'content'): # OpenAI
                        llm_full_text_response_snippet = response_obj.choices[0].message.content[:200]
                    elif hasattr(response_obj, 'content') and response_obj.content and hasattr(response_obj.content[0], 'text'): # Anthropic
                        llm_full_text_response_snippet = response_obj.content[0].text[:200]
                except Exception as e_text_extract:
                    print(f"Minor error while trying to extract raw LLM text for error reporting: {e_text_extract}")

                escaped_snippet = llm_full_text_response_snippet.replace('"', '\"').replace('\n', '\\n').replace("'", "\'")

                print(f"Warning: Could not extract valid Python code string from LLM response. parse_response returned: {type(policy_obj)}. LLM text snippet: '{escaped_snippet}'")
                python_script = f"print('Error: Could not extract Python code. LLM response might be missing valid code block or did not conform. Hint: {escaped_snippet}')"

            # Safeguard: ensure python_script is always a string
            if not isinstance(python_script, str):
                 python_script = "print('Error: Parsed script is not a string (safeguard hit).')"
            
            # Optional: keep LLM generated script print for debugging, or remove if too verbose
            # print(f"ConcreteLLMClient: LLM generated Python script:\n---\n{python_script}\n---") 
            return python_script
        except Exception as e:
            print(f"ConcreteLLMClient: Error during LLM call: {e}\n{traceback.format_exc()}")
            return f"print(f'Error during LLM call: {{e!r}}')" # Valid Python
    
    # The synchronous generate_script can also be updated if it's intended for use,
    # but the primary path for agents is async. For now, focusing on generate_script_async.
    # If generate_script (sync) is called, it might need similar system_prompt_override logic.
    def generate_script(self, prompt: str, system_prompt_override: Optional[str] = None) -> str:
        print("ConcreteLLMClient: generate_script (sync) called.")
        final_system_prompt = system_prompt_override if system_prompt_override is not None else self.default_system_prompt
        
        # Constructing a simplified user prompt for the sync version for now.
        # Ideally, it would also take user_prompt_content and system_prompt_override.
        # This sync wrapper is problematic and mostly for conceptual legacy.
        # The main change is to ensure it *could* pass a system prompt if acall was sync.
        
        user_content = prompt # Assuming 'prompt' is the full user content for sync version
        
        print(f"ConcreteLLMClient (sync): Using system prompt (first 100): '{final_system_prompt[:100]}...'")
        print(f"ConcreteLLMClient (sync): Using user content (first 100): '{user_content[:100]}...'")

        try:
            loop = asyncio.get_event_loop()
            if loop.is_running() and not hasattr(loop, '_nest_patched'): # Check if nest_asyncio is not used
                 # If in a running loop (e.g. Jupyter), and not patched for nesting,
                 # direct asyncio.run will fail. This path is hard to make robustly sync.
                 print("Warning: Sync call to async generate_script_async from a running event loop without nest_asyncio. This is likely to fail or deadlock.")
                 print("Returning error script. Please use 'await generate_script_async'.")
                 return "print('Error: Synchronous call to async method from running event loop without proper nesting. Use await.')"

            # If no loop or nest_asyncio is in play, asyncio.run might work.
            return asyncio.run(self.generate_script_async(user_prompt_content=user_content, system_prompt_override=final_system_prompt))
        except RuntimeError as e:
             print(f"RuntimeError in ConcreteLLMClient.generate_script (sync): {e}. Usually means asyncio.run() called from running loop.")
             return "print('Error: Could not run async generate_script synchronously due to event loop conflict. Use await.')"
        except Exception as e:
             print(f"General error in ConcreteLLMClient.generate_script (sync): {e}")
             return "print('Error: General error in sync call to async generate_script.')"

# Ensure other parts of the file (BaseLLMClient, PlaceholderLLMClient, imports) remain the same.
# The __init__ of ConcreteLLMClient is also updated to store a default_system_prompt.
if __name__ == '__main__':
    p_client = PlaceholderLLMClient()
    prompts_to_test = [
        "Mine 10 iron ore.",
        "Can you craft a stone furnace for me?",
    ]
    for p_prompt in prompts_to_test:
        p_script = p_client.generate_script(p_prompt) # This is sync
        print(f"\n--- Placeholder Test ---\nPrompt: {p_prompt}\nGenerated Script:\n{p_script}\n--------------------")

    async def main_test_concrete():
        try:
            # Check if the real LLMFactory was imported or if it's the placeholder
            if 'LLMFactory' in globals() and not hasattr(globals()['LLMFactory'], 'acall'): # Basic check for placeholder
                 print("\nSkipping ConcreteLLMClient test as real LLMFactory seems unavailable (using placeholder).")
                 return

            test_llm_factory = LLMFactory(model_name="mock-model-for-factory") # LLMFactory takes model_name
            
            concrete_client = ConcreteLLMClient(llm_factory=test_llm_factory, model_name="mock-model-for-client")
            script = await concrete_client.generate_script_async("Craft a wooden chest using real parse_response.")
            print(f"\n--- Concrete Client Async Test ---\nGenerated Script:\n{script}")
        except Exception as e:
            print(f"Error in main_test_concrete: {e}")
            traceback.print_exc()

    if hasattr(globals().get('LLMFactory'), 'acall'): # Only run if we have a somewhat real LLMFactory
        try:
            # Check if an event loop is already running. If so, schedule the test.
            # Otherwise, use asyncio.run(). This handles environments like Jupyter.
            loop = asyncio.get_event_loop()
            if loop.is_running():
                print("Asyncio loop already running. Scheduling main_test_concrete.")
                asyncio.ensure_future(main_test_concrete())
            else:
                # print("Running main_test_concrete with asyncio.run().")
                # asyncio.run(main_test_concrete()) # This will fail if API keys not set
                print("Skipping asyncio.run(main_test_concrete()) in this non-interactive environment to prevent blocking/errors if keys are missing.")
        except RuntimeError: # Fallback if get_event_loop() itself fails in some contexts
            # print("RuntimeError with event loop. Running main_test_concrete with asyncio.run().")
            # asyncio.run(main_test_concrete())
            print("Skipping asyncio.run(main_test_concrete()) in this non-interactive environment due to event loop issue.")

    else:
        print("\nConcreteLLMClient test setup skipped as real LLMFactory or parse_response might be missing.")

    print("\nTo fully test ConcreteLLMClient, ensure API keys are set and uncomment asyncio.run(main_test_concrete()) if appropriate for your environment.")
